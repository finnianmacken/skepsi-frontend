# Introduction

One of the first results in the mathematics of computation, which underlies the subsequent development of much of theoretical computer science, was the distinction between computable and non-computable functions shown in papers of Church [1936], Turing [1936], and Post [1936]. Central to this result is Church’s thesis, which says that all computing devices can be simulated by a Turing machine. This thesis greatly simplifies the study of computation, since it reduces the potential field of study from any of an infinite number of potential computing devices to Turing machines. Church’s thesis is not a mathematical theorem; to make it one would require a precise mathematical description of a computing device. Such a description, however, would leave open the possibility of some practical computing device which did not satisfy this precise mathematical description, and thus would make the resulting mathematical theorem weaker than Church’s original thesis.

With the development of practical computers, it has become apparent that the distinction between computable and non-computable functions is much too coarse; computer scientists are now interested in the exact efficiency with which specific functions can be computed. This exact efficiency, on the other hand, is too precise a quantity to work with easily. The generally accepted compromise between coarseness and precision distinguishes efficiently and inefficiently computable functions by whether the length of the computation scales polynomially  or superpolynomially with the input size. The class of problems which can be solved by algorithms having a number of steps polynomial in the input size is known as P.

For this classification to make sense, we need it to be machine-independent. That is, we need to know that whether a function is computable in polynomial time is independent of the kind of computing device used. This corresponds to the following quantitative version of Church’s thesis, which Vergis et al. [1986] have called the “Strong Church’s Thesis” and which makes up half of the “Invariance Thesis” of van Emde Boas [1990].

**Thesis** (Quantitative Church’s thesis): _Any physical computing device can be simulated by a Turing machine in a number of steps polynomial in the resources used by the computing device._

In statements of this thesis, the Turing machine is sometimes augmented with a random number generator, as it has not yet been determined whether there are pseudorandom number generators which can efficiently simulate truly random number generators for all purposes. Readers who are not comfortable with Turing machines may think instead of digital computers having an amount of memory that grows linearly with the length of the computation, as these two classes of computing machines can efficiently simulate each other.

There are two escape clauses in the above thesis. One of these is the word “physical.” Researchers have produced machine models that violate the above quantitative Church’s thesis, but most of these have been ruled out by some reason for why they are not “physical,” that is, why they could not be built and made to work. The other escape clause in the above thesis is the word “resources,” the meaning of which is not completely specified above. There are generally two resources which limit the ability of digital computers to solve large problems: time (computation steps) and space (memory). There are more resources pertinent to analog computation; some proposed analog machines that seem able to solve NP-complete problems in polynomial time have required the machining of exponentially precise parts, or an exponential amount of energy. (See Vergis et al. [1986] and Steiglitz [1988]; this issue is also implicit in the papers of Canny and Reif [1987] and Choi et al. [1995] on three-dimensional shortest paths.)

For quantum computation, in addition to space and time, there is also a third potentially important resource, precision. For a quantum computer to work, at least in any currently envisioned implementation, it must be able to make changes in the quantum states of objects (e.g., atoms, photons, or nuclear spins). These changes can clearly not be perfectly accurate, but must contain some small amount of inherent imprecision. If this imprecision is constant (i.e., it does not depend on the size of the input), then it is not known how to compute any functions in polynomial time on a quantum computer that cannot also be computed in polynomial time on a classical computer with a random number generator. However, if we let the precision grow polynomially in the input size (that is, we let the number of bits of precision grow logarithmically in the input size), we appear to obtain a more powerful type of computer. Allowing the same polynomial growth in precision does not appear to confer extra computing power to classical mechanics, although allowing exponential growth in precision does [Hartmanis and Simon 1974, Vergis et al. 1986].

As far as we know, what precision is possible in quantum state manipulation is dictated not by fundamental physical laws but by the properties of the materials and the architecture with which a quantum computer is built. It is currently not clear which architectures, if any, will give high precision, and what this precision will be. If the precision of a quantum computer is large enough to make it more powerful than a classical computer, then in order to understand its potential it is important to think of precision as a resource that can vary. Treating the precision as a large constant (even though it is almost certain to be constant for any given machine) would be comparable to treating a classical digital computer as a finite automaton — since any given computer has a fixed amount of memory, this view is technically correct; however, it is not particularly useful.

Because of the remarkable effectiveness of our mathematical models of computation, computer scientists have tended to forget that computation is dependent on the laws of physics. This can be seen in the statement of the quantitative Church’s thesis in van Emde Boas [1990], where the word “physical” in the above phrasing is replaced with the word “reasonable.” It is difficult to imagine any definition of “reasonable” in this context which does not mean “physically realizable,” i.e., that this computing machine could actually be built and would work.

Computer scientists have become convinced of the truth of the quantitative Church’s thesis through the failure of all proposed counter-examples. Most of these proposed counter-examples have been based on the laws of classical mechanics; however, the universe is in reality quantum mechanical. Quantum mechanical objects often behave quite differently from how our intuition, based on classical mechanics, tells us they should. It thus seems plausible that the natural computing power of classical mechanics corresponds to Turing machines[1], while the natural computing power of quantum mechanics might be greater.

The first person to look at the interaction between computation and quantum mechanics appears to have been Benioff \[1980, 1982a, 1982b\]. Although he did not ask whether quantum mechanics conferred extra power to computation, he showed that reversible unitary evolution was sufficient to realize the computational power of a Turing machine, thus showing that quantum mechanics is at least as powerful computationally as a classical computer. This work was fundamental in making later investigation of quantum computers possible.

Feynman \[1982,1986\] seems to have been the first to suggest that quantum mechanics might be more powerful computationally than a Turing machine. He gave arguments as to why quantum mechanics might be intrinsically expensive computationally to simulate on a classical computer. He also raised the possibility of using a computer based on quantum mechanical principles to avoid this problem, thus implicitly asking the converse question: by using quantum mechanics in a computer can you compute more efficiently than on a classical computer? Deutsch \[1985, 1989\] was the first to ask this question explicitly. In order to study this question, he defined both quantum Turing machines and quantum circuits and investigated some of their properties.

The question of whether using quantum mechanics in a computer allows one to obtain more computational power was more recently addressed by Deutsch and Jozsa \[1992\] and Berthiaume and Brassard \[1992a, 1992b\]. These papers showed that there are problems which quantum computers can quickly solve exactly, but that classical computers can only solve quickly with high probability and the aid of a random number generator. However, these papers did not show how to solve any problem in quantum polynomial time that was not already known to be solvable in polynomial time with the aid of a random number generator, allowing a small probability of error; this is the characterization of the complexity class BPP, which is widely viewed as the class of efficiently solvable problems.

Further work on this problem was stimulated by Bernstein and Vazirani \[1993\]. One of the results contained in their paper was an oracle problem (that is, a problem involving a “black box” subroutine that the computer is allowed to perform, but for which no code is accessible) which can be done in polynomial time on a quantum Turing machine but which requires super-polynomial time on a classical computer. This result was improved by Simon \[1994\], who gave a much simpler construction of an oracle problem which takes polynomial time on a quantum computer but requires *exponential* time on a classical computer. Indeed, while Bernstein and Vaziarni’s problem appears contrived, Simon’s problem looks quite natural. Simon’s algorithm inspired the work presented in this paper.

Two number theory problems which have been studied extensively but for which no polynomial-time algorithms have yet been discovered are finding discrete logarithms and factoring integers \[Pomerance 1987, Gordon 1993, Lenstra and Lenstra 1993, Adleman and McCurley 1995\]. These problems are so widely believed to be hard that several cryptosystems based on their difficulty have been proposed, including the widely used RSA public key cryptosystem developed by Rivest, Shamir, and Adleman \[1978\]. We show that these problems can be solved in polynomial time on a quantum computer with a small probability of error.

Currently, nobody knows how to build a quantum computer, although it seems as though it might be possible within the laws of quantum mechanics. Some suggestions have been made as to possible designs for such computers \[Teich et al. 1988, Lloyd 1993, 1994, Cirac and Zoller 1995, DiVincenzo 1995, Sleator and Weinfurter 1995, Barenco et al. 1995b, Chuang and Yamomoto 1995\], but there will be substantial difficulty in building any of these \[Landauer 1995a, Landauer 1995b, Unruh 1995, Chuang et al. 1995, Palma et al. 1995\]. The most difficult obstacles appear to involve the decoherence of quantum superpositions through the interaction of the computer with the environment, and the implementation of quantum state transformations with enough precision to give accurate results after many computation steps. Both of these obstacles become more difficult as the size of the computer grows, so it may turn out to be possible to build small quantum computers, while scaling up to machines large enough to do interesting computations may present fundamental difficulties.

Even if no useful quantum computer is ever built, this research does illuminate the problem of simulating quantum mechanics on a classical computer. Any method of doing this for an arbitrary Hamiltonian would necessarily be able to simulate a quantum computer. Thus, any general method for simulating quantum mechanics with at most a polynomial slowdown would lead to a polynomial-time algorithm for factoring.

The rest of this paper is organized as follows. In §2, we introduce the model of quantum computation, the *quantum gate array*, that we use in the rest of the paper. In  §§3 and 4, we explain two subroutines that are used in our algorithms: reversible modular exponentiation in §3 and quantum Fourier transforms in §4. In §5, we give our algorithm for prime factorization, and in §6, we give our algorithm for extracting discrete logarithms. In §7, we give a brief discussion of the practicality of quantum computation and suggest possible directions for further work.

# Quantum Computation
In this section we give a brief introduction to quantum computation, emphasizing the properties that we will use. We will describe only *quantum gate arrays*, or *quantum acyclic circuits*, which are analogous to acyclic circuits in classical computer science. For other models of quantum computers, see references on quantum Turing machines \[Deutsch 1989, Bernstein and Vazirani 1993, Yao 1993\] and quantum cellular automata \[Feynman 1986, Margolus 1986, 1990, Lloyd 1993, Biafore 1994\]. If they are allowed a small probability of error, quantum Turing machines and quantum gate arrays can compute the same functions in polynomial time \[Yao 1993\]. This may also be true for the various models of quantum cellular automata, but it has not yet been proved. This gives evidence that the class of functions computable in quantum polynomial time with a small probability of error is robust, in that it does not depend on the exact architecture of a quantum computer. By analogy with the classical class BPP, this class is called BQP.

Consider a system with *n* components, each of which can have two states. Whereas in classical physics, a complete description of the state of this system requires only n bits, in quantum physics, a complete description of the state of this system requires 2^n − 1 complex numbers. To be more precise, the state of the quantum system is a point in a 2n -dimensional vector space. For each of the 2n possible classical positions of the components, there is a basis state of this vector space which we represent, for example, by |011 · · · 0> meaning that the first bit is 0, the second bit is 1, and so on. Here, the *ket* notation |x>means that x is a (pure) quantum state. (Mixed states will

[1] I believe that this question has not yet been settled and is worthy of further investigation. See Vergis et al. \[1986\], Steiglitz \[1988\], and Rubel \[1989\]. In particular, turbulence seems a good candidate for a counterexample to the quantitative Church’s thesis because the non-trivial dynamics on many length scales may make it difficult to simulate on a classical computer
